			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Andrew Moreland <andymo@stanford.edu>
Alex Ryan <alexryan@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

We wrote two versions of our fixed point library. One is implemented using
a "fixed_point" struct in order to force the compiler to check types and
prevent us from accidentally mixing int operations with fp operations.
There is also a library implemented using macros which we tested after
ensuring that our code type checked properly.

Our code passes all tests when compiled on myth and executed without any opts.
If -r is used it consistently passes everything.
Some values of -j will cause failures in the MLPQS tests and in alarm_simultaneous.
The tests that fail all depend on very specific tick timings of threads being
awoken, or on the exact values of statistics about threads.
Since jitter adds a fairly extreme amount of randomness, there are times where
threads are awoken and then immediately pre-empted, and other such events
which will throw off these tests.

We read the BOCHS patch that added the -j option and noticed that the random
jitter is uniformly distributed, rather than normally distributed as one would
expect of real-world randomness in timings. This leads to a high incidence
of rapid preemptions particularly in the MLFQS tests because the interrupt code
(although optimized) still is fairly heavy, and it will eat up a large portion
of time slices that are very short.

Also, the MLFQS section is slightly underspecced as the precise ordering of
some computation is not stated. We chose to compute load average before
recent cpu before priority in cases where all three need computing.

Furthermore, the design docs do not specify what to set priority to when
creating a new thread in the MLFQS scheduler. We are told to ignore the
priority parameter, but not what to use instead. We have decided to use 
PRI_DEFAULT.


>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Andrew Moreland: Everything except fixed point 
Alex Ryan: Everything except timer 

Aside from that we sat next to eachother and wrote everything on one computer.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In a call to timer_sleep we block interrupts and then insert the thread into
our sleeping_threads list. The insertion is done in order to maintain 
the order of the list. The list is sorted so that the head of the list
is the thread that needs to wake up soonest. Then, we block.

When a timer interrupt occurs, it checks the head of the sleeping_threads
list. If it can awake the thread, it thread_unblocks it and then repeats.
When it finds a thread that needs to sleep for more time, it stop looking
and calls thread_tick.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We maintain the list in sorted order so that the interrupt handler only has a 
short O(1) operation to do unless there are threads that need waking. If there
are threads that need waking we only need to iterate forward in the list.


---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The primary race condition which we sought to avoid was the possibility
of contention for the sleeping_threads queue. In order to avoid this
we block interrupts which, since we are on a single core, prevents
other threads from calling timer_sleep when we are accessing the list.
The interrupt handler is also prevented from accessing the list because
interrupts are disabled.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Timer interrupts can only occur before we have disabled interrupts. 
We only modify shared state after we have disabled interrupts so
we are safe from race conditions there. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it is simple to implement and fast.
Also, the majority of the overhead is incurred outside of the 
interrupt handler -- insertion is O(n) but removal is O(1).

We considered keeping the list unsorted, which would allow faster
calls to thread_sleep, but this would add overhead in the interrupt
handler. We decided to optimize for shortening the time spent in the
interrupt handler.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

FIXME: TODO

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

FIXME: TODO 

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

A semaphore stores a list of the threads waiting on it. A lock has
a semaphore, and so transitively has this list. For these two cases
we iterate over the semaphore's waiting list and find the thread with
the highest priority whenever we need to wake a thread.

For cond variables we iterate over the semaphores that it holds.
Each semaphore only has one waiter, so we just find the semaphore
with the highest priority waiter and sema_up that semaphore.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When we call lock_acquire, we lock_try_acquire.
FIXME: todo

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

FIXME: todo

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

If we set a thread's priority and then are interrupted we could potentially
have left the ready lists in an inconsistent state. For instance, if the thread
was decreasing its own priority we might accidentally schedule it over
a higher priority thread.

We avoid this problem by disabling interrupts in the set_priority function.

We could kind of use a lock to avoid the race. We could lock the thread 
struct before modifications were made. However, we need to access the data
in timer interrupts in order to get the priority of the thread. We are
not allowed to truly acquire the lock in the timer interrupt as this could
block the interrupt and cause terrible, terrible problems. So, we could
*try*_acquire the lock. 

However, if we failed to acquire the lock our
only recourse would be to ignore the thread or not to schedule a new thread.
This is potentially exploitable by client threads -- they could predict when
they will be interrupted and initiate a priority change in order to block
the scheduler from pre-empting them. Or this behavior could happen on accident.
Either way, it would be bad.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We designed this way in order to minimize timer interrupt time and optimize
for a low lock-thread ratio. Since we are caching the max of the natural
and donated priorities, when the scheduler needs to (in timer interrupt context)
find the highest priority thread it need only traverse the sorted ready
lists which have already been fixed in order to take into account the cached
priorities.

FIXME: explain why we did a list of locks

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

FIXME: todo

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

The values in the table below make a few important assumptions:
- There is a different thread running (say, the 'main' thread)
before the first tick, and all threads are ready to run
- The initial load_avg is irrelevant; if all of these threads
start with a recent_cpu of 0, the initial load_avg calculation
at tick 0 (which wouldn't change until tick 100) will only be 
used to multiply recent_cpu during the first recent_cpu 
recalculation (which starts at 0).
- there are no other ready threads in the system
- as discussed in C3, when appropriate, we update the
load_avg, then recent_cpu, then priority.
- the calculations below represent the state of the system
*after* the specified tick

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   1   2  63  60  58      A
 4      4   1   2  62  60  58      A      
 8      8   1   2  61  60  58      A
12     12   1   2  60  60  58      B [round-robin]
16     12   5   2  60  59  58      A 
20     16   5   2  59  59  58      B [round-robin]
24     16   9   2  59  58  58      A
28     20   9   2  58  58  58      C [round-robin]
32     20   9   6  58  58  57      B [round-robin]
36     20  13   6  58  57  57      A

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Besides the ambiguities discussed above (which would be well-defined
in a real system) it was not stated the order in which load_average,
recent_cpu, and priority calculations should be done on one-second
clock boundaries. It seems to make the most sense to update them
in that order, as load_average is independent of the others, 
recent_cpu depends on only load_average, and priority depends in turn
on recent_cpu.

By updating them in their reverse-dependency order, the resulting
values should more immediately reflect the "snapshot" of the
system at that point in time.

This is the order that we adopted in our scheduler.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

As we have discussed before, we minimize the interrupt context
overhead for waking threads from sleep.

For the MLFQS scheduler we perform all statistical calculations 
within interrupt context (with the exception of recomputing a
single thread's priority when its nice value changes).

This is the only way that we saw to divide the work because
the computations must be carried out (according to spec) on
very specific timing frequencies and no kernel thread (or user thread)
is allowed to execute in between a tick and their computation.

However, computing the statistics could be fairly expensive on a loaded
system. We have one O(read_threads) operation and two O(threads)
operations with lots of non-trivial fixed-point computations.

In a heavily loaded system it is possible that this timer interrupt
scheduling overhead could cause time slices to be unreasonable short.

For non-MLFQS scheduling we only require the timer interrupt handler
to search sorted queues for the first entry. This is a very fast op
with low overheard.


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

FIXME: review this

We designed our MLFQS scheduler somewhat naively. We wrote it in a
way that allowed us to easily shoehorn it on top of the code 
that we wrote for the regular priority scheduler.

We chose to compute statistics for every thread when we were told.
There are certain optimizations that could make things faster.

For instance, we could track the tick when we last scheduled a given
thread. Then, instead of using expensive fixed point operations
in order to compute statistics for threads that have not been scheduled
recently, we could simply have set recent_cpu to 0 and made other similar
decisions.

We also perform some needless work when we decide whether or not we need
to yield to higher priority threads. We could optimize slightly and store
a reference to the current highest priority thread, and then looking
to see if we need to yield would only be O(1). This requires maintenance
of state, though, which is often the source of confusing bugs.

If we had more time to work on advanced scheduling it would be interesting
to explore priority donation for synchronization constructs, and in general
to find ways to avoid having to do expensive computations when they are
provably unhelpful.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

The first way we implemented it used a fixed_point *struct* that
contained the underlying int implementation. By wrapping the int
in a struct, we gain type-safety for all interface methods (that
is, we can't call fixed_point_add(2, 2) in an attempt to get 4).
We force the explicit conversion between fixed_point and int.
This definitely helped our development time, as we definitely
called the wrong versions of functions (or didn't perform the
appropriate conversions) which would have been completely horrible
to debug by hand.

However, in an attempt to lower time spent within the timer
interrupt, we added a second equivalent implementation that
simply typedefs int as fixed_point and exports the interface
as a collection of C macros. This guarantees function inlining
at the expense of type-safety. The implementations can be swapped through
a simple FIXED_POINT_INLINE flag. (Unfortunately, this optimization
did not improve the timing characteristics of the code significantly.)

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It would be nice to have a guide to what sorts of tests we could expect to
fail under jitter. I understand that we are supposed to use critical
thinking and figure it out, but it is stressful :-).

Also, the threshold for MLFQS_nice_10 is perhaps too tight. We wrote a 
simulator for mlfqs and we found that depending on very small 
flunctuations in starting conditions (small delays between starting threads,
etc.) we were able to produce reasonable outputs that were slightly outside
of the threshold using non-jitter timings.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

It is interesting that a lot of OS code does not use locks. It would be
neat to see how this works on a multi-core system.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

I think the notion of limiting the priority donation recursion freaks
some people out because the natural tree based design scales very well
to arbitrarily deep donation.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

Staffing office hours would probably be a good idea.

>> Any other comments?
