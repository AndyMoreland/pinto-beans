			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Andrew Moreland <andymo@stanford.edu>
Alex Ryan <alexryan@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

We wrote two versions of our fixed point library. One is implemented using
a "fixed_point" struct in order to force the compiler to check types and
prevent us from accidentally mixing int operations with fp operations.
There is also a library implemented using macros which we tested after
ensuring that our code type checked properly.

Our code passes all tests when compiled on myth and executed without any opts.
If -r is used it consistently passes everything.
Some values of -j will cause failures in the MLPQS tests and in alarm_simultaneous.
The tests that fail all depend on very specific tick timings of threads being
awoken, or on the exact values of statistics about threads.
Since jitter adds a fairly extreme amount of randomness, there are times where
threads are awoken and then immediately pre-empted, and other such events
which will throw off these tests.

We read the BOCHS patch that added the -j option and noticed that the random
jitter is uniformly distributed, rather than normally distributed as one would
expect of real-world randomness in timings. This leads to a high incidence
of rapid preemptions particularly in the MLFQS tests because the interrupt code
(although optimized) still is fairly heavy, and it will eat up a large portion
of time slices that are very short.

Also, the MLFQS section is slightly underspecced as the precise ordering of
some computation is not stated. We chose to compute load average before
recent cpu before priority in cases where all three need computing.

Furthermore, the design docs do not specify what to set priority to when
creating a new thread in the MLFQS scheduler. We are told to ignore the
priority parameter, but not what to use instead. We have decided to use 
PRI_DEFAULT.


>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Andrew Moreland: Everything except fixed point 
Alex Ryan: Everything except timer 

Aside from that we sat next to eachother and wrote everything on one computer.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In timer.c:

static struct list sleeping_threads

We use this static variable to store a list of threads that are currently
asleep, sorted from soonest to latest wake time.

In Thread.h's struct thread:

    int64_t sleep_until
    struct list_elem sleepelem

The thread is awoken by the timer interrupt when timer_ticks > sleep_until.
The thread is stored in a list called sleeping_threads. sleepelem is the 
element used for this.


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In a call to timer_sleep we block interrupts and then insert the thread into
our sleeping_threads list. The insertion is done in order to maintain 
the order of the list. The list is sorted so that the head of the list
is the thread that needs to wake up soonest. Then, we block.

When a timer interrupt occurs, it checks the head of the sleeping_threads
list. If it can awake the thread, it thread_unblocks it and then repeats.
When it finds a thread that needs to sleep for more time, it stop looking
and calls thread_tick.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We maintain the list in sorted order so that the interrupt handler only has a 
short O(1) operation to do unless there are threads that need waking. If there
are threads that need waking we only need to iterate forward in the list.


---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The primary race condition which we sought to avoid was the possibility
of contention for the sleeping_threads queue. In order to avoid this
we block interrupts which, since we are on a single core, prevents
other threads from calling timer_sleep when we are accessing the list.
The interrupt handler is also prevented from accessing the list because
interrupts are disabled.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Timer interrupts can only occur before we have disabled interrupts. 
We only modify shared state after we have disabled interrupts so
we are safe from race conditions there. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it is simple to implement and fast.
Also, the majority of the overhead is incurred outside of the 
interrupt handler -- insertion is O(n) but removal is O(1).

We considered keeping the list unsorted, which would allow faster
calls to thread_sleep, but this would add overhead in the interrupt
handler. We decided to optimize for shortening the time spent in the
interrupt handler.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h's struct thread:

    int native_priority
    struct lock *blocked_lock
    struct list locks


We use native priority to store the priority-without-donation of the thread.
We use "blocked_lock" to track which lock the thread is currently block on.
We use "locks" in order to track which locks we own currently. 
All of these are used for managing priority donation.

In thread.c:
static struct list ready_lists[PRI_COUNT]

We redefined ready_list to be an array of ready_lists -- each corresponds
to a different priority level.


In Synch.h's struct lock:
   struct list_elem elem;

This is a list_element used for membership in a thread's `locks` list.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Consider the situation diagrammed below, and explained underneath:

                          Thread3 (1)
                             ^
                             |
                             |           
                           /   \
                         /       \
                  { Lock1    ,    Lock2 }
                    ^                 ^
                    |                 |
                   /                   \
                 / \                   / \
      { Thread1a , Thread1b } { Thread2a,Thread2b }
          (2)        (4)          (3) ^      (6)
                                       \
                                   { Lock4 }

There are two locks, Lock1 and Lock2. Both are owned by Thread3,
represented as an arrow in the dependency graph. There are two threads
Thread1a and Thread1b waiting on the Lock1 that Thread3 holds; likewise,
two threads Thread2a and Thread2b are blocked on Lock2. Thread 2a also
holds some Lock4. "Native" priorities (that is, assigned priorities 
before any donation) are represented in parentheses.

This information is encoded in lists; each collection inside {} braces
represent a list owned by its parent node; that is, Thread3 keeps track
of all locks that it owns, { Lock1, Lock2 }. Lock1 keeps track of
all threads waiting on it, { Thread1a, Thread1b }. Each node also knows
its parent (locks know which thread currently owns them; threads know
which lock, if any, they are blocked on). All of these lists are 
unsorted.

Now, donated priority is accessible for any thread: a thread's priority
is calculated separate from its "native" priority and is exactly the max
of any threads waiting on the locks that it owns. In the diagram above,
Thread3's actual priority will be max(max(2, 4), max(3, 6)) = 6.
We cache this value on the thread.

If Thread 3 releases Lock2, it will be acquired by Thread2b. The
only updating that has to be done is that Lock2 is owned by Thread2b
(and Thread2b owns Lock 2). We already know that 
priority(Thread2b) > priority(Thread2a) because it acquired the lock,
so no priority recalculation has to be done for either of those two
threads. However, we must recalculate Thread 3's priority because it
no longer owns the high-priority Lock2. This is accomplished by just
iterating as in the previous paragraph (foreach owned lock, foreach 
blocked thread, take the maximum). The result is max(2,4) = 4.

In the other interesting case in the diagram above, say a high-priority 
Thread4 tries to acquire Lock4. It must now recursively climb up the 
dependency graph, donating priority to Thread2a, and finally to Thread3.
If at any point this recursion meets a parent of at least equal priority,
the recursion stops as all subsequent parents are already high-priority.


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

A semaphore stores a list of the threads waiting on it. A lock has
a semaphore, and so transitively has this list. For these two cases
we iterate over the semaphore's waiting list and find the thread with
the highest priority whenever we need to wake a thread.

For cond variables we iterate over the semaphores that it holds.
Each semaphore only has one waiter, so we just find the semaphore
with the highest priority waiter and sema_up that semaphore.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When we call lock_acquire, we lock_try_acquire. If this succeeds then
we do not need to donate any priority to anything as we already have
the lock and are not blocked.

If, on the other hand, this fails, then we need to donate priority
to the holder of the lock. We do this by adding ourself to the lock's
semaphore's waiter list and then recursively traversing the dependency
graph through the owner of the lock and any threads who it is depending
on. We only raise priorities during this traversal, terminating if
at any point we hit a thread whose priority is equal or higher to
our own as we know that all following threads will have the same
property.

This takes care of nested donation.

These operations all take place with interrupts off in order to avoid
complicated race conditions.

Once we get the lock we add it to our "owned locks" list while 
interrupts are off.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
With interrupts off:

When lock_release is called we first remove the lock from our
"owned_locks" list. Then, we iterate over the remaining locks in
our "owned_locks" list in order to figure out if we are still
having priority donated to us. We update our priority appropriately
either to the max of our natural priority and the max of any
dependant threads' priorities.

Then, we iterate over the lock's semaphore's waiters list and find the
thread with the highest cached priority. We unblock this thread.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

If we set a thread's priority and then are interrupted we could potentially
have left the ready lists in an inconsistent state. For instance, if the thread
was decreasing its own priority we might accidentally schedule it over
a higher priority thread.

We avoid this problem by disabling interrupts in the set_priority function.

We could kind of use a lock to avoid the race. We could lock the thread 
struct before modifications were made. However, we need to access the data
in timer interrupts in order to get the priority of the thread. We are
not allowed to truly acquire the lock in the timer interrupt as this could
block the interrupt and cause terrible, terrible problems. So, we could
*try*_acquire the lock. 

However, if we failed to acquire the lock our
only recourse would be to ignore the thread or not to schedule a new thread.
This is potentially exploitable by client threads -- they could predict when
they will be interrupted and initiate a priority change in order to block
the scheduler from pre-empting them. Or this behavior could happen on accident.
Either way, it would be bad.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

One of our main goals with the design was the easy recomputation of
donated priorities; in the case of a lock changing hands, the thread
that released the lock very likely might lose priority. We need some
mechanism to calculate the lowered priority, which we accomplished by
providing access to all dependant threads (that is, threads that are
waiting on locks held by the thread in question).

One simple solution would be for a thread to directly keep track of *all*
dependant threads (recursively). However, the tree structure optimizes
over this naive solution as a parent's priority is always at least as 
big as any children (and we are interested in calculating the max).

But a thread keeping track of dependant children threads is 
clunky when a lock changes hands; if n threads are all waiting on a 
single lock L held by T, then T releases and thread S acquires L, 
the other n-1 have to cut themselves out of the children list of T and 
add themselves to the children list of S. This is an O(n) operation, so 
the successive acquisition of the lock by all n threads takes O(n^2) time.

By keeping track of dependant threads per-lock, this scenario translates
to simply removing the lock L from T's list and adding it to S's (as well
as a pointer reassignment). This is O(1).

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h's struct thread:

  int nice
  fixed_point recent_cpu

We use `nice` in order to track the nice level of a thread.
We use `recent_cpu` in order to track the recent cpu of a thread.

In thread.c:

static fixed_point load_average

We store a thread.c-global variable that represent's the system's 
current load average.

All of these values are used for MLFQS computation.

In fixed-point.h: 

typedef struct { int impl_value; } fixed_point

We defined a wrapper struct type for fixed-point computation.
We used a struct in order to make the compiler type-check our math.
We also provide a macro implementation that uses no struct.


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

The values in the table below make a few important assumptions:
- There is a different thread running (say, the 'main' thread)
before the first tick, and all threads are ready to run
- The initial load_avg is irrelevant; if all of these threads
start with a recent_cpu of 0, the initial load_avg calculation
at tick 0 (which wouldn't change until tick 100) will only be 
used to multiply recent_cpu during the first recent_cpu 
recalculation (which starts at 0).
- there are no other ready threads in the system
- as discussed in C3, when appropriate, we update the
load_avg, then recent_cpu, then priority.
- the calculations below represent the state of the system
*after* the specified tick

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   1   2  63  60  58      A
 4      4   1   2  62  60  58      A      
 8      8   1   2  61  60  58      A
12     12   1   2  60  60  58      B [round-robin]
16     12   5   2  60  59  58      A 
20     16   5   2  59  59  58      B [round-robin]
24     16   9   2  59  58  58      A
28     20   9   2  58  58  58      C [round-robin]
32     20   9   6  58  58  57      B [round-robin]
36     20  13   6  58  57  57      A

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Besides the ambiguities discussed above (which would be well-defined
in a real system) it was not stated the order in which load_average,
recent_cpu, and priority calculations should be done on one-second
clock boundaries. It seems to make the most sense to update them
in that order, as load_average is independent of the others, 
recent_cpu depends on only load_average, and priority depends in turn
on recent_cpu.

By updating them in their reverse-dependency order, the resulting
values should more immediately reflect the "snapshot" of the
system at that point in time.

This is the order that we adopted in our scheduler.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

As we have discussed before, we minimize the interrupt context
overhead for waking threads from sleep.

For the MLFQS scheduler we perform all statistical calculations 
within interrupt context (with the exception of recomputing a
single thread's priority when its nice value changes).

This is the only way that we saw to divide the work because
the computations must be carried out (according to spec) on
very specific timing frequencies and no kernel thread (or user thread)
is allowed to execute in between a tick and their computation.

However, computing the statistics could be fairly expensive on a loaded
system. We have one O(read_threads) operation and two O(threads)
operations with lots of non-trivial fixed-point computations.

In a heavily loaded system it is possible that this timer interrupt
scheduling overhead could cause time slices to be unreasonable short.

For non-MLFQS scheduling we only require the timer interrupt handler
to search sorted queues for the first entry. This is a very fast op
with low overheard.


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

We designed our MLFQS scheduler somewhat naively. We wrote it in a
way that allowed us to easily shoehorn it on top of the code 
that we wrote for the regular priority scheduler.

We chose to compute statistics for every thread when we were told.
There are certain optimizations that could make things faster.

For instance, we could track the tick when we last scheduled a given
thread. Then, instead of using expensive fixed point operations
in order to compute statistics for threads that have not been scheduled
recently, we could simply have set recent_cpu to `nice` and made other
similar decisions.

We avoided making these optimizations both because they are complicated
and because they could potentially cause us to fail the very specifically
designed tests for this project.

If we had more time to work on advanced scheduling it would be interesting
to explore priority donation for synchronization constructs, and in general
to find ways to avoid having to do expensive computations when they are
provably unhelpful.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

The first way we implemented it used a fixed_point *struct* that
contained the underlying int implementation. By wrapping the int
in a struct, we gain type-safety for all interface methods (that
is, we can't call fixed_point_add(2, 2) in an attempt to get 4).
We force the explicit conversion between fixed_point and int.
This definitely helped our development time, as we definitely
called the wrong versions of functions (or didn't perform the
appropriate conversions) which would have been completely horrible
to debug by hand.

However, in an attempt to lower time spent within the timer
interrupt, we added a second equivalent implementation that
simply typedefs int as fixed_point and exports the interface
as a collection of C macros. This guarantees function inlining
at the expense of type-safety. The implementations can be swapped through
a simple FIXED_POINT_INLINE flag. (Unfortunately, this optimization
did not improve the timing characteristics of the code significantly.)

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It would be nice to have a guide to what sorts of tests we could expect to
fail under jitter. I understand that we are supposed to use critical
thinking and figure it out, but it is stressful :-).

Also, the threshold for MLFQS_nice_10 is perhaps too tight. We wrote a 
simulator for mlfqs and we found that depending on very small 
flunctuations in starting conditions (small delays between starting threads,
etc.) we were able to produce reasonable outputs that were slightly outside
of the threshold using non-jitter timings.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

It is interesting that a lot of OS code does not use locks. It would be
neat to see how this works on a multi-core system.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

I think the notion of limiting the priority donation recursion freaks
some people out because the natural tree based design scales very well
to arbitrarily deep donation.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
